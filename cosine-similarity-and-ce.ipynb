{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport numpy as np\nimport torch.nn as nn\nfrom torch.autograd.function import Function\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torchvision.transforms as transforms\nfrom  torch.utils.data import DataLoader\nimport torch.optim.lr_scheduler as lr_scheduler\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T15:57:19.802961Z","iopub.execute_input":"2022-05-10T15:57:19.803304Z","iopub.status.idle":"2022-05-10T15:57:21.762317Z","shell.execute_reply.started":"2022-05-10T15:57:19.803218Z","shell.execute_reply":"2022-05-10T15:57:21.761560Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### DATASET ###\n\n!wget www.di.ens.fr/~lelarge/MNIST.tar.gz\n!tar -zxvf MNIST.tar.gz\nroot_dir = './'\n\n# DOWNLOAD TRAINING SET WITH CLASS 0-8\n\ntrainset = datasets.MNIST(root=root_dir, download=True,train=True, transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))]))\n\nclass YourSampler(torch.utils.data.sampler.Sampler):\n    def __init__(self, mask, data_source):\n        self.mask = mask\n        self.data_source = data_source\n\n    def __iter__(self):\n        return iter([i.item() for i in torch.nonzero(mask)])\n\n    def __len__(self):\n        return len(self.data_source)\n\nmask = [0 if (trainset[i][1] == 9) else 1 for i in range(len(trainset))]\nmask = torch.tensor(mask)  \nsampler = YourSampler(mask, trainset)\n\ntrain_loader = DataLoader(trainset, batch_size=128, sampler = sampler, num_workers=2)  \n\n# DOWNLOAD TESING SET WITH CLASS 0-9\n\ntestset = datasets.MNIST(root=root_dir, download=True,train=False, transform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.1307,), (0.3081,))]))\n\ntest_loader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)  ","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:21.764102Z","iopub.execute_input":"2022-05-10T15:57:21.764367Z","iopub.status.idle":"2022-05-10T15:57:36.207360Z","shell.execute_reply.started":"2022-05-10T15:57:21.764314Z","shell.execute_reply":"2022-05-10T15:57:36.206527Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"--2022-05-10 15:57:22--  http://www.di.ens.fr/~lelarge/MNIST.tar.gz\nResolving www.di.ens.fr (www.di.ens.fr)... 129.199.99.14\nConnecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://www.di.ens.fr/~lelarge/MNIST.tar.gz [following]\n--2022-05-10 15:57:22--  https://www.di.ens.fr/~lelarge/MNIST.tar.gz\nConnecting to www.di.ens.fr (www.di.ens.fr)|129.199.99.14|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: unspecified [application/x-gzip]\nSaving to: ‘MNIST.tar.gz’\n\nMNIST.tar.gz            [             <=>    ]  33.20M  11.7MB/s    in 2.8s    \n\n2022-05-10 15:57:26 (11.7 MB/s) - ‘MNIST.tar.gz’ saved [34813078]\n\nMNIST/\nMNIST/raw/\nMNIST/raw/train-labels-idx1-ubyte\nMNIST/raw/t10k-labels-idx1-ubyte.gz\nMNIST/raw/t10k-labels-idx1-ubyte\nMNIST/raw/t10k-images-idx3-ubyte.gz\nMNIST/raw/train-images-idx3-ubyte\nMNIST/raw/train-labels-idx1-ubyte.gz\nMNIST/raw/t10k-images-idx3-ubyte\nMNIST/raw/train-images-idx3-ubyte.gz\nMNIST/processed/\nMNIST/processed/training.pt\nMNIST/processed/test.pt\n","output_type":"stream"}]},{"cell_type":"code","source":"### BASIC NET ###\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1_1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n        self.prelu1_1 = nn.PReLU()\n        self.conv1_2 = nn.Conv2d(32, 32, kernel_size=5, padding=2)\n        self.prelu1_2 = nn.PReLU()\n        self.conv2_1 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n        self.prelu2_1 = nn.PReLU()\n        self.conv2_2 = nn.Conv2d(64, 64, kernel_size=5, padding=2)\n        self.prelu2_2 = nn.PReLU()\n        self.conv3_1 = nn.Conv2d(64, 128, kernel_size=5, padding=2)\n        self.prelu3_1 = nn.PReLU()\n        self.conv3_2 = nn.Conv2d(128, 128, kernel_size=5, padding=2)\n        self.prelu3_2 = nn.PReLU()\n        self.preluip1 = nn.PReLU()\n        self.ip1 = nn.Linear(128*3*3, 2)\n\n    def forward(self, x):\n        x = self.prelu1_1(self.conv1_1(x))\n        x = self.prelu1_2(self.conv1_2(x))\n        x = F.max_pool2d(x,2)\n        x = self.prelu2_1(self.conv2_1(x))\n        x = self.prelu2_2(self.conv2_2(x))\n        x = F.max_pool2d(x,2)\n        x = self.prelu3_1(self.conv3_1(x))\n        x = self.prelu3_2(self.conv3_2(x))\n        x = F.max_pool2d(x,2)\n        x = x.view(-1, 128*3*3)\n        ip1 = self.preluip1(self.ip1(x))\n        return ip1","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:36.208970Z","iopub.execute_input":"2022-05-10T15:57:36.210690Z","iopub.status.idle":"2022-05-10T15:57:36.222755Z","shell.execute_reply.started":"2022-05-10T15:57:36.210657Z","shell.execute_reply":"2022-05-10T15:57:36.221053Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### CUSTOM COSINE SIMILARITY DENSE ###\n\nclass CosineSimilarityDense(nn.Module):\n\n    def __init__(self, inputs, outputs):\n        super().__init__()\n        self.outputs = outputs\n        self.weight = nn.Parameter(torch.randn(inputs, outputs))\n        \n    def forward(self, x):\n        linear = torch.matmul(x, self.weight/torch.norm(self.weight) )\n        \n        return F.log_softmax(linear)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:36.224999Z","iopub.execute_input":"2022-05-10T15:57:36.225726Z","iopub.status.idle":"2022-05-10T15:57:36.238734Z","shell.execute_reply.started":"2022-05-10T15:57:36.225690Z","shell.execute_reply":"2022-05-10T15:57:36.238046Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"### COMPLETE NET ###\n\nclass Conv2D_Cos(nn.Module):\n    def __init__(self):\n        super(Conv2D_Cos, self).__init__()\n        self.net = Net()\n        self.cos_dense = CosineSimilarityDense(2,9)\n\n    def forward(self, x):\n        x = self.net(x)\n        x = self.cos_dense(x)\n        \n        return x\n\n    def get_embeddings(self, x):\n        x = self.net(x)\n\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:36.239994Z","iopub.execute_input":"2022-05-10T15:57:36.241473Z","iopub.status.idle":"2022-05-10T15:57:36.248506Z","shell.execute_reply.started":"2022-05-10T15:57:36.241431Z","shell.execute_reply":"2022-05-10T15:57:36.247737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"### TRAINING FUNCTIONS/LOOPS ###\n\ndef Cos_training(dataloader, model, optimizer):\n    size = len(dataloader.dataset)\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    correct = 0.0\n    loss_sum = 0.0\n    cnt_batches = 0\n\n    for batch, (X, y) in enumerate(dataloader):\n\n        X, y = X.to(device), y.to(device)\n        cnt_batches += 1\n    \n        # Backpropagation\n        optimizer.zero_grad()\n        \n        # Compute prediction and loss\n        preds = model(X.float().to(device))\n       \n        loss = CE_loss(preds, y.long())\n\n        loss.backward()\n        optimizer.step()\n        \n        correct += (preds.argmax(1) == y).type(torch.float).sum().item()\n        loss_sum += loss\n        \n        if batch % 20 == 0:\n            loss, current = loss.item(), batch * len(X)\n    \n    # training accuracy\n    correct /= size\n    loss /= cnt_batches\n\n    print(f\"Training Error:  Accuracy: {(100*correct):>0.1f}%\")\n    loss = loss.to('cpu')\n    \n    return float(loss.detach().numpy()), correct","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:36.249772Z","iopub.execute_input":"2022-05-10T15:57:36.250625Z","iopub.status.idle":"2022-05-10T15:57:36.260961Z","shell.execute_reply.started":"2022-05-10T15:57:36.250588Z","shell.execute_reply":"2022-05-10T15:57:36.260295Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"### Softmax/CE loss FUNCTION ###\ndef CE_loss(prediction, label):\n    CEL =  torch.nn.CrossEntropyLoss()\n    \n    return CEL(prediction, label)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:36.262108Z","iopub.execute_input":"2022-05-10T15:57:36.262965Z","iopub.status.idle":"2022-05-10T15:57:36.268928Z","shell.execute_reply.started":"2022-05-10T15:57:36.262871Z","shell.execute_reply":"2022-05-10T15:57:36.268231Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"### TRAIN THE NETWORK ###\n\nuse_cuda = torch.cuda.is_available() and True\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n# Model\nmodel = Conv2D_Cos().to(device)\n\n# optimzer\noptimizer = optim.SGD(model.parameters(),lr=0.001,momentum=0.9, weight_decay=0.0005)\nsheduler = lr_scheduler.StepLR(optimizer,20,gamma=0.8)\n\n# Training with 50 epoches\nfor epoch in range(50):\n    sheduler.step()\n    print(f\"Epoch: : {epoch}\")\n    Cos_training(train_loader, model, optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T15:57:36.270300Z","iopub.execute_input":"2022-05-10T15:57:36.270876Z","iopub.status.idle":"2022-05-10T16:06:49.264811Z","shell.execute_reply.started":"2022-05-10T15:57:36.270841Z","shell.execute_reply":"2022-05-10T16:06:49.263961Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Epoch: : 0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  del sys.path[0]\n","output_type":"stream"},{"name":"stdout","text":"Training Error:  Accuracy: 9.8%\nEpoch: : 1\nTraining Error:  Accuracy: 17.4%\nEpoch: : 2\nTraining Error:  Accuracy: 18.3%\nEpoch: : 3\nTraining Error:  Accuracy: 20.2%\nEpoch: : 4\nTraining Error:  Accuracy: 44.1%\nEpoch: : 5\nTraining Error:  Accuracy: 56.2%\nEpoch: : 6\nTraining Error:  Accuracy: 68.3%\nEpoch: : 7\nTraining Error:  Accuracy: 77.3%\nEpoch: : 8\nTraining Error:  Accuracy: 82.8%\nEpoch: : 9\nTraining Error:  Accuracy: 84.5%\nEpoch: : 10\nTraining Error:  Accuracy: 85.6%\nEpoch: : 11\nTraining Error:  Accuracy: 86.4%\nEpoch: : 12\nTraining Error:  Accuracy: 86.9%\nEpoch: : 13\nTraining Error:  Accuracy: 87.3%\nEpoch: : 14\nTraining Error:  Accuracy: 87.6%\nEpoch: : 15\nTraining Error:  Accuracy: 87.9%\nEpoch: : 16\nTraining Error:  Accuracy: 88.1%\nEpoch: : 17\nTraining Error:  Accuracy: 88.2%\nEpoch: : 18\nTraining Error:  Accuracy: 88.3%\nEpoch: : 19\nTraining Error:  Accuracy: 88.6%\nEpoch: : 20\nTraining Error:  Accuracy: 88.9%\nEpoch: : 21\nTraining Error:  Accuracy: 89.2%\nEpoch: : 22\nTraining Error:  Accuracy: 89.5%\nEpoch: : 23\nTraining Error:  Accuracy: 89.6%\nEpoch: : 24\nTraining Error:  Accuracy: 89.7%\nEpoch: : 25\nTraining Error:  Accuracy: 89.6%\nEpoch: : 26\nTraining Error:  Accuracy: 89.7%\nEpoch: : 27\nTraining Error:  Accuracy: 89.8%\nEpoch: : 28\nTraining Error:  Accuracy: 89.8%\nEpoch: : 29\nTraining Error:  Accuracy: 89.7%\nEpoch: : 30\nTraining Error:  Accuracy: 89.7%\nEpoch: : 31\nTraining Error:  Accuracy: 89.6%\nEpoch: : 32\nTraining Error:  Accuracy: 89.6%\nEpoch: : 33\nTraining Error:  Accuracy: 89.6%\nEpoch: : 34\nTraining Error:  Accuracy: 89.7%\nEpoch: : 35\nTraining Error:  Accuracy: 89.7%\nEpoch: : 36\nTraining Error:  Accuracy: 89.8%\nEpoch: : 37\nTraining Error:  Accuracy: 89.8%\nEpoch: : 38\nTraining Error:  Accuracy: 89.8%\nEpoch: : 39\nTraining Error:  Accuracy: 89.8%\nEpoch: : 40\nTraining Error:  Accuracy: 89.9%\nEpoch: : 41\nTraining Error:  Accuracy: 90.0%\nEpoch: : 42\nTraining Error:  Accuracy: 90.0%\nEpoch: : 43\nTraining Error:  Accuracy: 90.0%\nEpoch: : 44\nTraining Error:  Accuracy: 90.1%\nEpoch: : 45\nTraining Error:  Accuracy: 90.1%\nEpoch: : 46\nTraining Error:  Accuracy: 90.1%\nEpoch: : 47\nTraining Error:  Accuracy: 90.1%\nEpoch: : 48\nTraining Error:  Accuracy: 90.1%\nEpoch: : 49\nTraining Error:  Accuracy: 90.1%\n","output_type":"stream"}]},{"cell_type":"code","source":"### EXTRACT THE CLASS SCORE ###\n\ntotal_value = []\n\nfor data_train in train_loader:\n    \n    images,real_labels = data_train\n    images = images.to(device)\n    real_labels = real_labels.to(device)\n    \n    output = model(images)\n    \n    # get the output value for all train data\n    value, position = torch.max(output,1)\n    value = value.tolist()\n    total_value.extend(value)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:06:49.267698Z","iopub.execute_input":"2022-05-10T16:06:49.267959Z","iopub.status.idle":"2022-05-10T16:06:57.501097Z","shell.execute_reply.started":"2022-05-10T16:06:49.267924Z","shell.execute_reply":"2022-05-10T16:06:57.500273Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  del sys.path[0]\n","output_type":"stream"}]},{"cell_type":"code","source":"### DETERMINE THE OOD DETECTION THRESHOLD BY SORTING\n\ntotal_value.sort()\nid = int(0.01 * len(total_value))\nthreshold = total_value[id]\n\nprint(f\"The threshold by sorting is: {threshold}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:06:57.502798Z","iopub.execute_input":"2022-05-10T16:06:57.503047Z","iopub.status.idle":"2022-05-10T16:06:57.514378Z","shell.execute_reply.started":"2022-05-10T16:06:57.503011Z","shell.execute_reply":"2022-05-10T16:06:57.512520Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"The threshold by sorting is: -0.022744616493582726\n","output_type":"stream"}]},{"cell_type":"code","source":"### USE MODEL AND THRESHOLD ON TEST DATA ###\n\ncorrect = 0\ntotal = 0\ni = 0\ns = []\nlabels_pre = []\n\nfor data_test in test_loader:\n    images,real_labels = data_test\n    images = images.to(device)\n    real_labels = real_labels.to(device)\n    \n    output = model(images)\n    \n    value, position = torch.max(output,1)\n    labels_pre.append((position))\n    \n    # get the ID of predicted ood samples and save in s\n    y = torch.zeros(value.shape).to(value.device) \n    y[value < threshold] = 1\n    id = (y==1).nonzero(as_tuple = False).cpu()\n\n    ood = i*128+id\n    ood = (ood).numpy().flatten()\n    s = np.concatenate((s,ood))\n    \n    i = i + 1\n    correct += (position == real_labels).sum()\n    \ns = s.astype(int)  \nprint(f\"The overall accuracy before detection is: {(1/100*correct):>0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:06:57.515636Z","iopub.execute_input":"2022-05-10T16:06:57.515903Z","iopub.status.idle":"2022-05-10T16:06:59.393313Z","shell.execute_reply.started":"2022-05-10T16:06:57.515866Z","shell.execute_reply":"2022-05-10T16:06:59.392438Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:13: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n  del sys.path[0]\n","output_type":"stream"},{"name":"stdout","text":"The overall accuracy before detection is: 87.8%\n","output_type":"stream"}]},{"cell_type":"code","source":"### MARK THE DETECTED OOD SAMPLES AS 9 ###\n\nlabels = torch.cat(labels_pre, 0)\n\nfor i in range(len(s)):\n    j = s[i]\n    labels[j] = 9","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:06:59.394914Z","iopub.execute_input":"2022-05-10T16:06:59.395455Z","iopub.status.idle":"2022-05-10T16:06:59.419057Z","shell.execute_reply.started":"2022-05-10T16:06:59.395413Z","shell.execute_reply":"2022-05-10T16:06:59.418377Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"### EVALUATE THE RESULTS ###\n\n# Get the number of in and out of distribution samples\n\ntestset.targets = testset.targets.to(device)\ny = torch.zeros(testset.targets.shape).to(testset.targets.device) \ny[testset.targets == 9] = 1\nnum_ood = (y==1).nonzero(as_tuple = False).cpu()\nnum_ind = (y!=1).nonzero(as_tuple = False).cpu()\n\n# Percentage of OOD samples detected as OOD samples \n\nood_acc = 0\n\nfor k in range(len(testset.targets)):\n    if testset.targets[k]==9:\n            ood_acc += (testset.targets[k] == labels[k])\n\nood_acc =ood_acc / len(num_ood) \nprint(f\"The accuracy of out of distribution detection: \\n Accuracy is: {(100*ood_acc):>0.1f}%\")\n\n# Percentage of in-distribution samples that were detected as in-distribution  \n\nind_acc = 0\n\nfor k in range(len(testset.targets)):\n    if testset.targets[k]!=9:\n            ind_acc += (testset.targets[k] == labels[k])\n\nind_acc =ind_acc / len(num_ind) \nprint(f\"The accuracy of in distribution detection: \\n Accuracy is: {(100*ind_acc):>0.1f}%\")\n\n# Evaluate the overall accuracy\n\noverall_acc = 0\n\nfor k in range(len(testset.targets)):\n    overall_acc += (testset.targets[k] == labels[k])\noverall_acc = overall_acc/len(testset.targets) \nprint(f\"The overall accuracy after ood detection: \\n Accuracy is: {(100*overall_acc):>0.1f}%\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T16:06:59.420292Z","iopub.execute_input":"2022-05-10T16:06:59.420623Z","iopub.status.idle":"2022-05-10T16:07:00.335101Z","shell.execute_reply.started":"2022-05-10T16:06:59.420586Z","shell.execute_reply":"2022-05-10T16:07:00.334359Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"The accuracy of out of distribution detection: \n Accuracy is: 62.8%\nThe accuracy of in distribution detection: \n Accuracy is: 95.4%\nThe overall accuracy after ood detection: \n Accuracy is: 92.1%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}